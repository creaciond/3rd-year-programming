{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задача обучения линейного классификатора\n",
    "\n",
    "### Дано:\n",
    "Обучающая выборка $X^l = (x_i, y_i)^l_{i=1}$\n",
    "\n",
    "$x_i$ $-$ объекты, векторы из множества $\\mathcal{R}^{n}$\n",
    "\n",
    "$y_i$ $-$ метки классов, элементы множества $Y=\\{-1, 1\\}$\n",
    "### Найти:\n",
    "\n",
    "Параметры $w\\in \\mathcal{R}^n$, $w_0\\in \\mathcal{R}$ линейной модели классификации\n",
    "$$\n",
    "a(x; w, w_0) = \\mathrm{sign}\\left(\\langle w, x\\rangle - w_0\\right)\n",
    "$$\n",
    "\n",
    "### Критерий:\n",
    "Минимизация эмпирического риска:\n",
    "$$\n",
    "\\sum_{i=1}^{l}[a(x; w, w_0) \\not=y_i] = \\sum_{i=1}^{l}[M_i(w, w_0) < 0] \\to \\min_{w, w0}\n",
    "$$\n",
    "\n",
    "$M_i(w, w_0) = \\left(\\langle w, x\\rangle - w_0\\right)y_i$ $-$ отступ объекта $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Аппроксимация и регуляризация эмпирического риска\n",
    "\n",
    "Эмперический риск $-$ кусочно-постоянная разрывная функция\n",
    "\n",
    "Заменим его оценкой сверху, непрерывной по параметрам:\n",
    "\n",
    "$$\n",
    "Q(w, w_0) = \\sum_{i=1}^{l}[M_i(w, w_0) < 0] \\leq \\\\\n",
    "\\leq \\sum_{i=1}^{l}(1 - M_i(w, w_0))_{+} + \\frac{1}{2C}||w||^2 \\to \\min_{w, w_0}\n",
    "$$\n",
    "\n",
    "- Аппроксимация штрафует объекты за приближение к границе классов, увеличивая зазор между классами\n",
    "- Регуляризация штрафует неустойчивые решения в случае мультиколлинеарности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Оптимальная разделяющая гиперплоскость\n",
    "\n",
    "Линейный классификатор: $a(x; w, w_0) = \\mathrm{sign}\\left(\\langle w, x\\rangle - w_0\\right)\n",
    "$\n",
    "\n",
    "Пусть выборка линейно разделима:\n",
    "$$\n",
    "\\exists w, w_0: \\ M_{i}(w, w_0) = \\left(\\langle w, x\\rangle - w_0\\right)y_i > 0, \\ \\forall i=1\\dots l\n",
    "$$\n",
    "\n",
    "Нормировка $\\min_{i=1\\dots l} M_i(w, w0) = 1$\n",
    "\n",
    "Разделяющая полоса (разделяющая гиперплоскость посередине):\n",
    "$$\n",
    "\\{x: -1 \\leq\\langle w, x\\rangle - w_0 \\leq1\\} \\\\\n",
    "\\exists x_{+}: \\langle w, x_+\\rangle - w_0 = +1 \\\\ \n",
    "\\exists x_{-}: \\langle w, x_-\\rangle - w_0 = -1\n",
    "$$\n",
    "\n",
    "Ширина полосы:\n",
    "\n",
    "$$\n",
    "\\frac{\\langle x_+ - x_-, w\\rangle}{||w||} = \\frac{2}{||w||} \\to \\max\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обоснование кусочно-линейной функции потерь\n",
    "\n",
    "Линейно разделимая выборка (hard margin svm):\n",
    "\\begin{cases}\n",
    "&\\frac{1}{2}||w||^2 \\to \\min_{w, w_0} \\\\\n",
    "&M_i(w, w_0)\\geq 1, \\ i = 1,\\dots, l\n",
    "\\end{cases}\n",
    "\n",
    "Переход к линейно неразделимой выборке (soft margin svm):\n",
    "\n",
    "\\begin{cases}\n",
    "&\\frac{1}{2}||w||^2 + C\\sum_{i=1}^l\\xi_i\\to \\min_{w, w_0, \\xi} \\\\\n",
    "&M_i(w, w_0)\\geq 1 - \\xi_i, \\ i = 1,\\dots, l \\\\\n",
    "&\\xi_i\\geq 0, \\ i = 1,\\dots, l\n",
    "\\end{cases}\n",
    "\n",
    "Эквивалентна задаче безусловной оптимизации:\n",
    "\n",
    "$$\n",
    "C\\sum_{i=1}^{l}(1 - M_i(w, w_0))_+ + \\frac{1}{2}||w||^2 \\to \\min_{w, w0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Условия Каруша-Куна-Таккера\n",
    "\n",
    "Задача математического программирования:\n",
    "\\begin{cases}\n",
    "f(x) \\to \\min_x \\\\\n",
    "g_i(x)\\leq0, \\ i=1,\\dots,m \\\\\n",
    "h_j(x) = 0, \\ j=1, \\dots, n \\\\\n",
    "\\end{cases}\n",
    "\n",
    "Необъодимые условия. Если $x$ $-$ точка локального минимума,\n",
    "то существуют множители $\\mu_i, i=1,\\dots,m$ и $\\lambda_j, j=1,\\dots,n$:\n",
    "\n",
    "\\begin{cases}\n",
    "\\dfrac{\\partial L}{\\partial x} = 0, \\ L(x;\\mu,\\lambda) = f(x) + \\sum_{i=1}^{m}\\mu_i g_i(x) + \\sum_{j=1}^n\\lambda_jh_j \\\\\n",
    "g_i(x)\\leq 0, \\ h_j = 0 \\ \\text{(исходные ограничения)} \\\\\n",
    "\\mu_i\\geq 0 \\ \\text{(двойственные ограничения)} \\\\\n",
    "\\mu_ig_i = 0 \\ \\text{(условие дополняющей нежесткости)}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Применение условий ККТ к задаче SVM\n",
    "\n",
    "Функция Лагранжа:\n",
    "$$\n",
    "L = \\frac{1}{2}||w||^2 - \\sum_{i=1}^{l}\\lambda_i(M_i(w,w_0) - 1) - \\sum_{i=1}^l\\xi_i(\\lambda_i+\\eta_i -C)\n",
    "$$\n",
    "\n",
    "$\\lambda_i$ $-$ переменные, двойственные к ограничениям $M_i\\geq1-\\xi_i$\n",
    "\n",
    "$\\lambda_i$ $-$ переменные, двойственные к ограничениям $\\xi_i\\geq0$\n",
    "\n",
    "\\begin{cases}\n",
    "\\dfrac{\\partial L}{\\partial w} = 0, \\ \\dfrac{\\partial L}{\\partial w_0} = 0, \\ \\dfrac{\\partial L}{\\partial \\xi} = 0 \\\\\n",
    "\\xi_i \\geq 0, \\ \\lambda_i \\geq 0, \\ \\eta_i\\geq0, \\ i=1\\dots l \\\\\n",
    "\\lambda_i=0 \\ \\text{или} \\ M_i(w, w_0)=1-\\xi_i, \\ i = 1\\dots l \\\\\n",
    "\\eta_i=0 \\ \\text{или} \\ \\xi_i=0, \\ i=1\\dots l\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Необходимые условия седловой точки функции Лагранжа\n",
    "\n",
    "Функция Лагранжа:\n",
    "$$\n",
    "L = \\frac{1}{2}||w||^2 - \\sum_{i=1}^{l}\\lambda_i(M_i(w,w_0) - 1) - \\sum_{i=1}^l\\xi_i(\\lambda_i+\\eta_i -C)\n",
    "$$\n",
    "\n",
    "Необходимые условия седловой точки функции Лагранжа в случае hard-margin SVM:\n",
    "\n",
    "\\begin{cases}\n",
    "\\dfrac{\\partial L}{\\partial w} = 0 , \\ w = \\sum_{i=1}^l\\lambda_i y_i x_i \\\\\n",
    "\\dfrac{\\partial L}{\\partial w_0} = 0, \\ \\ \\ \\sum_{i=1}^l\\lambda_iy_i = 0, \\ i = 1\\dots l \\\\\n",
    "\\dfrac{\\partial L}{\\partial \\xi_i} = 0, \\ \\ \\ \\eta_i + \\lambda_i = C, \\ i = 1\\dots l\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Понятие опорного вектора\n",
    "\n",
    "1. $\\lambda_i = 0$, $\\eta_i=C$, $\\xi_i=0$, $M_i\\geq1$ $-$ переферийные (неинформативные) объекты.\n",
    "2. $0<\\lambda_i<C$, $0<\\eta_i<С$, $\\xi_i=0$, $M_i=1$ $-$ опорные граничные объекты.\n",
    "3. $0<\\lambda_i = C$, $\\eta_i=0$, $\\xi_i>0$, $M_i<1$ $-$ опорные нарушители объекты.\n",
    "\n",
    "Объект $x_i$ называется опорным, если $\\lambda_i\\not=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Двойственная задача\n",
    "\n",
    "\\begin{cases}\n",
    "-L(\\lambda) = -\\sum_{i=1}^l{\\lambda_i} + \\dfrac{1}{2}\\sum_{i=1}^l\\sum_{j=1}^l\\lambda_i\\lambda_jy_iy_j\\langle x_i, x_j\\rangle \\\\\n",
    "0\\geq \\lambda_i \\geq C, \\ i = 1, \\dots, l \\\\\n",
    "\\sum_{i=1}^l\\lambda_iy_i=0\n",
    "\\end{cases}\n",
    "\n",
    "Решение прямой задачи выражается через решение двойственной задачи:\n",
    "\\begin{cases}\n",
    "w = \\sum_{i=1}^l\\lambda_iy_ix_i \\\\\n",
    "w_0 = \\langle w, x_i \\rangle - y_i, \\ \\forall i: \\lambda_i>0, M_i = 1.\n",
    "\\end{cases}\n",
    "\n",
    "Итоговый линейный классификатор:\n",
    "$$\n",
    "a(x) = \\mathrm{sign}\\left(\\sum_{i=1}^l\\lambda_iy_i\\langle x_i, x\\rangle - w_0\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Резюме:\n",
    "\n",
    "- SVM $-$ лучший метод линейной классификации\n",
    "- SVM $-$ лекго обобщается для нелинейной классификации, для линейной и нелинейной регрессии\n",
    "- Аппроксимация пороговой функции потерь увеличивает зазор и поваышает качество классификации\n",
    "- Регуляризация устраняет мультиколлинеарность и уменьшает переобучение\n",
    "- Регуляризация эквивалентна введению априорного распределения в пространстве коэффициентов\n",
    "- $L_1$ и другие нестандарнтные регуляризаторы делают отбор признаков без явного пребора подмножеств"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Полезные ссылки\n",
    "\n",
    "[видео лекция Воронцова в ШАД](https://www.youtube.com/watch?v=Adi67_94_gc)\n",
    "\n",
    "[pdf лекция от Воронцова](http://www.ccas.ru/voron/download/SVM.pdf)\n",
    "\n",
    "[notebook examples from Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)\n",
    "\n",
    "[python examples](https://sadanand-singh.github.io/posts/svmpython/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
